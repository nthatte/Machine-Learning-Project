\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
<<<<<<< HEAD
\renewcommand\refname{Papers To Read}
\title{	}
=======
\usepackage{amsmath}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}
\renewcommand\refname{Papers To Read}
\title{Semantic Segmentation Using Hybrid Markov Logic Networks}
>>>>>>> 11f719ccc9fa4b78837c28e7aca0d94fdaa870b1

\author{
Aravindh Mahendran \\
\texttt{amahend1@andrew.cmu.edu} \\ 
\And
Nitish Thatte \\
\texttt{nitisht@andrew.cmu.edu} \\
\AND
Adwait Gandhe \\
\texttt{agandhe@andrew.cmu.edu} \\
}

<<<<<<< HEAD
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle
\vspace{-0.5in}
\section{Project Idea}
\vspace{-0.125in}
\label{sec:projidea}
Social networks allow users to share photographs, tag them and organize them into albums (defined by Meta-labels, eg: Birthday). The task of creating albums from unorganized images is currently left to the user and is very tedious. We propose to build a semi-automated photo organization system that learns from collective and individual user input and provides personalized categorization of photographs. 

We use existing label and meta-label data to learn a global prior model and user specific models of the correlation between labels and meta-labels. The former allows us to make predictions for users who haven't created the concerned meta-label(or album) and the later allows us to make even better predictions for users who have created these categories and pre-populated them with a few relevant images. Our algorithm uses the users' evaluations of these predictions to iteratively improve the global and user specific models, and thus refine the predictions.
\vspace{-0.125in}
\section{Dataset}
\vspace{-0.125in}
We plan to use the MIR FLIKR 1M dataset. The dataset consists of images, user defined tags and user identification numbers. 
We use user defined tags to approximate the output of an object recognition algorithm that would otherwise generate these tags from images.
The dataset is designed to contain, for each user, for each (manually chosen) meta-label , a set of images and their corresponding low level labels. We use this form of the data to simulate N users in the semi-supervised learning process. 
%
%The simulation begins by assigning meta-labels to a subset of the images. These are used by our algorithm to make predictions for the images without meta-labels. A random subset of these are then marked as correct or incorrect by the simulation engine and this data is then incorporated by our algorithm in an attempt to refine all the unmarked predictions. This repeats for a finite number of iterations after which the system stops to avoid over fitting.
\vspace{-0.125in}
\section{Software}
\vspace{-0.125in}
To complete this project we will write software that (a) parses the dataset to extract metadata, (b) runs the multi-user simulation and (c) implements the learning algorithm discussed in section \ref{sec:projidea}.
\vspace{-0.125in}
\section{Midway Report Milestone}
\vspace{-0.125in}
In the midway report we shall evaluate different approaches of machine learning to learn the global model and write software that parses the dataset and runs the simulation. Each team member shall experiment with a different machine learning technique on a toy subset of the complete dataset. Further, Adwait will write the parsing software; Nitish will write the simulation software and Aravindh will discuss testing approaches feasible for the evaluation of the algorithms.

\nocite{personalizingimagesearchresults}
\nocite{lornet}
\nocite{Sebastiani:2002:MLA:505282.505283}


\bibliography{bibtex.bib}
=======
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy

\begin{document}
\maketitle
\section{Project Idea}
\vspace{-0.125in}

The task of semantic labeling in crowded scenes is a difficult problem that has received much attention from the machine learning, computer vision and robotics communities. 
Previous approaches include Stacked Hierarchical Labeling \cite{Munoz:2010:SHL:1888212.1888218} and taking advantage of visual word co-occurrence using hierarchical vocabulary trees \cite{Micusik08032012}. 
We propose a new approach that uses hybrid Markov logic networks (HMLNs) \cite{Wang:2008:HML:1620163.1620244} to capture relationships between adjacent super-pixels and class co-occurrence.
This approach relaxes the I.I.D assumption used by some of the previous techniques and allows us to use first order logic to express complex relationships between super-pixels.

Examples of predicate statements we may use include $isClass(superPixel, Class)$ and $neighbors(superPixel, superPixel)$. Possible formulas based on these predicates are, $isClass(s,t)$, $neighbors(s,s') \Rightarrow \left( isClass(s, t) \Leftrightarrow isClass(s', t) \right)$. The former will learn the prior probability of a given class type based on the training data and the later will learn the influence a neighboring label on the label assigned to a given super-pixel. 
	
\vspace{-0.125in}
\section{Dataset}
\vspace{-0.125in}
We plan to use the CAMVID dataset (Cambridge-Driving Labeled Video Database), which consists of videos and user defined object class semantic labels filmed from a car driving through a city.
This dataset includes 32 semantic classes such as buildings, trees, and sidewalks. 

\vspace{-0.125in}
\section{Software}
\vspace{-0.125in}
Alchemy is an open source software developed by University of Washington that implements efficient learning and inference algorithms for HMLNs. We are required to write the predicates/numeric terms and formulas/numeric properties for the task of semantic segmentation. The formulas/numeric properties would decide the kind of relationships we learn. The HMLN inference would operate over image features describing super pixel appearance information. Software for over segmentation and feature extraction would also be written by us and may use libraries such as OpenCV.

\vspace{-0.125in}
\section{Midway Report Milestone}
\vspace{-0.125in}
From now until the midterm report, Nitish will work on writing HMLNs, Aravindh will work on feature extraction and Adwait will work on super-pixel generation techniques. The midterm report will consist of the results of initial experiments with different techniques for each aspect and present our design choices.

\vspace{-.125in}
\bibliography{../bibtex}
>>>>>>> 11f719ccc9fa4b78837c28e7aca0d94fdaa870b1
\bibliographystyle{plain}
\end{document}
