\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{comment}
\usepackage[top=1.0in, bottom=1.0in, left=1.0in, right=1.0in]{geometry}

%\renewcommand\refname{Papers To Read}
\title{Semantic Segmentation Using Hybrid Markov Logic Networks}

\author{
Aravindh Mahendran \\
\texttt{amahend1@andrew.cmu.edu} \\ 
\And
Nitish Thatte \\
\texttt{nitisht@andrew.cmu.edu} \\
\AND
Adwait Gandhe \\
\texttt{agandhe@andrew.cmu.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy

\begin{document}
\maketitle

\begin{abstract}
Semantic image segmentation is the process of assigning human relevant labels to pixels in an image and is a high level vision problem. Increasing the number of labels increases the complexity of the problem. Markov Logic Networks (MLNs) allow us to handle uncertainty and complexity in a single framework, whereas Hybrid Markov Logic Networks (HMLNs) are an extension of the MLNs that allow continuous properties and functions over those properties as features.  In this paper, we propose a method for semantic segmentation using Hybrid Markov Logic Networks, which integrate first order logic and statistical learning. We test our results on the CAMVID dataset (Cambridge-Driving Labeled Video Database).  
\end{abstract}


\section{Introduction}
\label{sec:Intro}
Semantic segmentation is the process of assigning a class label to each pixel of the image. This is an important problem in computer vision for understanding the underlying information in an image. While classical segmentation techniques group together the pixels based on low level features, semantic segmentation adopts a supervised learning approach. There are two common approaches for semantic segmentation. The first makes use of low level features and combines them with a learning framework to obtain higher level labels. The second approach is to use low level cues, rather than features, with random fields and learn a unified framework using low level segmentation. In this paper we explore the second approach further by combining logical and statistical techniques to jointly address the issues of uncertainty and complexity. 

This paper is structured as follows: Section \ref{sec:Related} discusses the related work. In section \ref{sec:Attempt}, we discuss methods that we have attempted for semantic segmentation. The experiments conducted and the results obtained are presented in section \ref{sec:Exp}. Section \ref{sec:New} presents our approach for the second half of the semester. We summarize our conclusions in section \ref{sec:Conclusion}. And finally section \ref{sec:Plan} outlines our plan to implement the proposed approach.

%------------------------------------------------------------------------------------------

\section{Related Work}
\label{sec:Related}

One of the first approaches for simultaneous object segmentation and recognition utilizes an Implicit Shape Model that integrates both capabilities into a common probabilistic framework \cite{Leibe04combinedobject}. 
Another approach used a generative model based on the bag of words representation for such simultaneous recognition and segmentation \cite{cao:spatially}.  
Furthermore, a method based on some of these approaches for scoring low-level patches according to their class relevance and propagating these posterior probabilities to pixels has been developed \cite{conf/bmvc/CsurkaP08}.

Several recent approaches use random fields to incorporate local cues and impart global control without implementing low level segmentation. For example, \cite{Kumar:2005:OC:1068507.1068889} proposes a Bayesian method for combining top-down and bottom-up cues. Conditional random fields have also been used for this purpose \cite{Kumar:2005:HFF:1097115.1097790} \cite{Richard04multiscaleconditional}. Finally, Textonboost \cite{Shotton06textonboost:joint} is an approach to learning a discriminative model of object classes incorporating appearance, shape and context information efficiently. We suggest \cite{SegmentRegionsParts} for other semantic image segmentation approaches.

These approaches handle the complexity and uncertainty inherent in the structured inference problem in different ways. An alternate approach is to use Markov Logic Networks \cite{Domingos06unifyinglogical} \cite{Richardson06markovlogic} that attach weights to first-order formulae and view them as templates for building Markov Networks. Hybrid Markov Logic Networks (HMLNs)\cite{Wang_hybridmarkov} are an extension that allow for continuous properties to appear as features. In this paper we discuss our attempts to use HMLNs for semantic segmentation. We discuss below the three approaches that inspired our attempted method.  

\textbf{Supervised Geodesic Propagation for Semantic Label Transfer} \hfill \\
\label{sec:Geo}
\noindent A recent approach for semantic segmentation is to use a learned label transfer confidence function to propagate labels between neighboring superpixels based on the geodesic distance metric \cite{Chen2012}. Labels are iteratively propagated based on geodesic distance from seeds to to other super pixels.
%TODO
This paper employs several useful tools for image segmentation. First its use of superpixels, which are groups of similar pixels in terms of both position and appearance, allows represents the important information in the images in a compressed and easy to process manner. Second, constructing graphs of superpixels allowed the authors to express how and where labels can propagate. Finally, propagating labels iteratively allows one to obtain a semantic segmentation without needing to minimize a cost function.

\textbf{Semantic Image Segmentation using Iterative Context Forests} \hfill \\
\label{sec:RandFor}
\noindent Another recent approach learns a single random forest and incrementally adds context features derived from coarser levels. Unlike Textonboost \cite{Shotton06textonboost:joint}, which learns two random forests, this approach models the dependencies between contextual and non-contextual features directly. The authors suggest that learning the contextual and image features in a single framework is beneficial as it better reflects the inherent dependency between the two types of features. This work uses a large number of features in a single iterative framework and provides better results than previous approaches.

\textbf{Semantic Image Segmentation using Bag of Words Approach} \hfill \\
\label{sec:bagofwords}
\noindent Semantic image segmentation has also been approached using the bag of words model. In one of the recent approaches, \cite{visualdictrene}, the authors develop a conditional random field model that combines dictionary learning, feature categorization (assigning key points to visual words) and image semantic segmentation into one framework. The conditional random field consists of energy terms for bottom up segmentation, key point categorization, dictionary learning and visual word co-occurrence.
%------------------------------------------------------------------------------------------

\section{Attempted Methods}
\label{sec:Attempt}
%------------------------------------------------------------------------------------------
\subsection{Label Propagation}
\label{sec:AttemptLabProp}

	In section \ref{sec:Geo}, we discussed a method of propagating labels on a graph of connected superpixels based on a learned label transfer confidence function. This method introduced us to two ideas: propagating labels rather than directly minimizing a cost function to arrive at a segmentation and working with graphs of related superpixels. Building on these ideas, we learn the weights for a hybrid Markov logic network in order to refine a prior expected segmentation obtained from a baseline algorithm. The method incorporates an undirected graph of connected superpixels that expresses where labels can propagate.

%TODO
	Using a MLN provides several advantages. First, MLNs allows us to perform metric learning by discovering a different weight for every label and feature vector component combination. Therefore, an MLN can model complex systems more easily than an SVM or logistic regression, which can learn one weight vector for all labels. Another advantage of MLNs is that they allow the user to express a template for his or her model using first order logic, a language that is easy to understand and learn.

Numeric terms and predicates in this Markov logic network are: $\mathrm{featureDistance}(s_i,s_j)$, $\mathrm{isLabel}(s_i,l_k)$, and $\mathrm{isNeighbor}(s_i,s_j)$.
Where $s_i$ is the $i^\textrm{th}$ superpixel in an image and $l_k$ is the $k^\textrm{th}$ label. 
$\mathrm{featureDistance}()$ is a numeric term that returns the computed distance between two image feature vectors via a distance metric such as euclidean or cosine distance. $\mathrm{isLabel}()$ returns 1 if superpixel $s_i$ has label $l_j$ and 0 otherwise, and $\mathrm{isNeighbor}()$ returns 1 if its arguments are the neighbors and 0 otherwise. 

An example of a formula in this hybrid Markov logic network that use these predicates and numeric terms are :

\begin{equation*}
	[isNeighbor(s_i,s_j) \Rightarrow (isLabel(s_i,l_k) \Leftrightarrow isLabel(s_k,l_m))]*(featureDistance(s_i, s_k, f_m))
\end{equation*}

This formula encodes the following two beliefs about how and why labels can propagate in the image:	Superpixels with similar feature vectors should have the same label and superpixels that are connected by an edge (are neighbors) should have the same label.

A brute-force approach to this problem would be to allow every super pixel to transfer its label to every other superpixel. This would result in a complete graph, where there is an edge between every two distinct nodes. Such a graph would allow superpixel labels to be learned from a more global context in the image. However, there would be an immense number of formulas for which to learn weights, making finding the solution computationally intractable. 

In contrast, a simple approach is to connect nodes that are directly share borders. Consequently, the graph would very few edges, greatly reducing the number of required learned weights and simplifying the learning problem. This approach, however, would only use local context for label propagation.

Therefore, we propose to take a middle ground approach that will produce densely connected superpixels where relations are expected to be strong, and fewer edges across boundaries expected to not share labels. In order build this superpixel graph, we take two levels of superpixel segmentations: A coarse level, which we will henceforth refer to as supersuperpixels, and a fine level, which we term superpixels. These segmentations were found using the algorithm outlined in \cite{Superpixel}. All superpixels within a supersuperpixel are connected allowing for contextual information to be shared within a region. Additionally superpixels that share a boundary between two supersuperpixels are connected allowing for labels to propagate across supersuperpixels while acknowledging that label transfer across supersuperpixels is less probable than within a supersuperpixel.

We then search through the training data set and select the $k-NN$ according to the distance with respect to the GIST Features \cite{Gist} of the images. We use the superpixels in these $k$ images to train the MLN on the formulas listed abo

\subsection{Iterative Context Forests}
\label{sec:IterContFor}

In the section \ref{sec:RandFor} we discussed a method of using large number of features to learn a single random forest for semantic label segmentation in a joint, flexible and fast manner. The motivation to use HMLN comes with the same purpose of learning the context and pixel features at the same time. We attempt to learn a HMLN on such multiple feature sets, however, while the random forests select a subset of the feature set, the HMLN weights are trained while working with all these features simultaneously. We realize while conducting experiments using \cite{alchemy} that the process of learning the weights for the HMLN is extremely time consuming and memory intensive and hence we abandon this approach. 

\subsection{Bag of Words Approach}
\label{sec:BofWordApp}

In section \ref{sec:bagofwords} we discussed the bag of words approach for semantic image segmentation. Even though the bag of words approach developed in \cite{visualdictrene} has shown very good results on the CAMVID and Graz-02 \cite{graz02} datasets it cannot be incorporated into a MLN as it requires second order logic. Particularly, the word count histogram built as part of the bag of words model requires a count of the number of true predicates of a certain kind which is a meta level information beyond the first order logic. Hence, we abandon this approach.

\section{Experiment}
\label{sec:Exp}

We have used the Alchemy - Open Source AI software \cite{alchemy} for learning the weights of the HMLN.

\begin{figure}
	\centering
	\begin{comment}
		\begin{subfigure}[c]{\textwidth}
			\centering
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/1_22_s.png}
				\label{fig:1_22_s}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/1_22_s_GT.png}
				\label{fig:1_22_s_lab}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/1_22_s_map.png}
				\label{fig:1_22_s_map}
			\end{subfigure}
			\begin{subfigure}[]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/su1_22_s.pdf}
				\label{fig:1_22_s_su}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/adj1_22_s.pdf}
				\label{fig1_22_s_adj}
			\end{subfigure}
		\end{subfigure}
	\end{comment}

	\begin{subfigure}[c]{\textwidth}
		\centering
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/2_21_s.png}
			\label{fig:2_21_s}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/2_21_s_GT.png}
			\label{fig:2_21_s_lab}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/2_21_s_map.png}
			\label{fig:2_21_s_map}
		\end{subfigure}
		\begin{subfigure}[]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/su2_21_s.pdf}
			\label{fig:2_21_s_su}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/adj2_21_s.pdf}
			\label{fig2_21_s_adj}
		\end{subfigure}
	\end{subfigure}

	\begin{subfigure}[c]{\textwidth}
		\centering
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/4_1_s.png}
			\label{fig:4_1_s}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/4_1_s_GT.png}
			\label{fig:4_1_s_lab}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/4_1_s_map.png}
			\label{fig:4_1_s_map}
		\end{subfigure}
		\begin{subfigure}[]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/su4_1_s.pdf}
			\label{fig:4_1_s_su}
		\end{subfigure}
		\begin{subfigure}[c]{0.195\textwidth}
			\includegraphics[width = \textwidth]{./img/adj4_1_s.pdf}
			\label{fig4_1_s_adj}
		\end{subfigure}
	\end{subfigure}
	
	\begin{comment}
		\begin{subfigure}[c]{\textwidth}
			\centering
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/6_3_s.png}
				\label{fig:6_3_s}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/6_3_s_GT.png}
				\label{fig:6_3_s_lab}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/6_3_s_map.png}
				\label{fig:6_3_s_map}
			\end{subfigure}
			\begin{subfigure}[]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/su6_3_s.pdf}
				\label{fig:6_3_s_su}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/adj6_3_s.pdf}
				\label{fig6_3_s_adj}
			\end{subfigure}
		\end{subfigure}

		\begin{subfigure}[c]{\textwidth}
			\centering
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/7_8_s.png}
				\label{fig:7_8_s}

			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/7_8_s_GT.png}
				\label{fig:7_8_s_lab}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/7_8_s_map.png}
				\label{fig:7_8_s_map}
			\end{subfigure}
			\begin{subfigure}[]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/su7_8_s.pdf}
				\label{fig:7_8_s_su}
			\end{subfigure}
			\begin{subfigure}[c]{0.195\textwidth}
				\includegraphics[width = \textwidth]{./img/adj7_8_s.pdf}
				\label{fig7_8_s_adj}
			\end{subfigure}
		\end{subfigure}

	\end{comment}	
	\begin{subfigure}[t]{\textwidth}
		\centering
		\begin{subfigure}[t]{0.195\textwidth}
			\subcaption{original image}
		\end{subfigure}
		\begin{subfigure}[t]{0.195\textwidth}
			\subcaption{ground truth segmentation}
		\end{subfigure}
		\begin{subfigure}[t]{0.195\textwidth}
			\subcaption{visual words map}
		\end{subfigure}
		\begin{subfigure}[t]{0.195\textwidth}
			\subcaption{superpixel segmentation}
		\end{subfigure}
		\begin{subfigure}[t]{0.195\textwidth}
			\subcaption{superpixel adjacency matrix}
		\end{subfigure}
	\end{subfigure}
	\caption{}
\end{figure}
%------------------------------------------------------------------------------------------
\section{New Proposed Method}
\label{sec:New}
%------------------------------------------------------------------------------------------

\section{Conclusion}
\label{sec:Conclusion}
%------------------------------------------------------------------------------------------
\section{Plan of Activities}
\label{sec:Plan}

%------------------------------------------------------------------------------------------\bibliography{midway_report}
\bibliographystyle{plain}
\bibliography{midway_report}

\end{document}
